{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-11-8a6f574cbaf1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-8a6f574cbaf1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    `python -m spacy download en`\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "`python -m spacy download en`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en',disable=['parser', 'tagger','ner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.max_length = 1198623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_punc(doc_text):\n",
    "    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-/:;<=>?@[\\\\]^_`{|}~\\t\\n ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filepath):\n",
    "    with open(filepath) as f:\n",
    "        str_text = f.read()\n",
    "    return str_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = read_file('./data/moby_dick_chapters(1-4).txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = separate_punc(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "11394"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 25\n",
    "train_len = sequence_len + 1\n",
    "text_sequences =[]\n",
    "for i in range(train_len,len(tokens)):\n",
    "    seq = tokens[i-train_len:i]\n",
    "    text_sequences.append(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode sequences of words\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "964 : call\n14 : me\n265 : ishmael\n51 : some\n263 : years\n416 : ago\n87 : never\n222 : mind\n129 : how\n111 : long\n962 : precisely\n262 : having\n50 : little\n43 : or\n37 : no\n321 : money\n7 : in\n23 : my\n555 : purse\n3 : and\n150 : nothing\n261 : particular\n6 : to\n2704 : interest\n14 : me\n24 : on\n"
    }
   ],
   "source": [
    "for i in sequences[0]:\n",
    "    print(f'{i} : {tokenizer.index_word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2709"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 964,   14,  265, ..., 2704,   14,   24],\n       [  14,  265,   51, ...,   14,   24,  965],\n       [ 265,   51,  263, ...,   24,  965,    5],\n       ...,\n       [ 960,   12,  168, ...,  264,   53,    2],\n       [  12,  168, 2703, ...,   53,    2, 2709],\n       [ 168, 2703,    3, ...,    2, 2709,   26]])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sequences = np.array(sequences)\n",
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,LSTM,Embedding\n",
    "\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n",
    "    model.add(LSTM(150, return_sequences=True))\n",
    "    model.add(LSTM(150))\n",
    "    model.add(Dense(150, activation='relu'))\n",
    "\n",
    "    model.add(Dense(vocabulary_size, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "   \n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sequences[:,:-1] \n",
    "y = sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=vocabulary_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len = X.shape[1]\n",
    "seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "WARNING:tensorflow:From C:\\Users\\Ariel\\Anaconda3\\envs\\nlp\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 25, 25)            67750     \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 25, 150)           105600    \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 150)               180600    \n_________________________________________________________________\ndense_1 (Dense)              (None, 150)               22650     \n_________________________________________________________________\ndense_2 (Dense)              (None, 2710)              409210    \n=================================================================\nTotal params: 785,810\nTrainable params: 785,810\nNon-trainable params: 0\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "265 - acc: 0.3052\nEpoch 107/300\n11368/11368 [==============================] - 18s 2ms/step - loss: 2.7925 - acc: 0.3087\nEpoch 108/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.7718 - acc: 0.3172\nEpoch 109/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.7511 - acc: 0.3212\nEpoch 110/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.7372 - acc: 0.3271\nEpoch 111/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.7656 - acc: 0.3246\nEpoch 112/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.8174 - acc: 0.3112\nEpoch 113/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.7502 - acc: 0.3205\nEpoch 114/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.7188 - acc: 0.3269\nEpoch 115/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.6744 - acc: 0.3348\nEpoch 116/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.6524 - acc: 0.3390\nEpoch 117/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.6287 - acc: 0.3429\nEpoch 118/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.6167 - acc: 0.3476\nEpoch 119/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.6030 - acc: 0.3531\nEpoch 120/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.5913 - acc: 0.3556\nEpoch 121/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.5732 - acc: 0.3558\nEpoch 122/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.5664 - acc: 0.3571\nEpoch 123/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.5362 - acc: 0.3661\nEpoch 124/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 2.5213 - acc: 0.3681\nEpoch 125/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4997 - acc: 0.3712\nEpoch 126/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4868 - acc: 0.3767\nEpoch 127/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4728 - acc: 0.3818\nEpoch 128/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4623 - acc: 0.3855\nEpoch 129/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4465 - acc: 0.3847\nEpoch 130/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 2.4304 - acc: 0.3843\nEpoch 131/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4255 - acc: 0.3868\nEpoch 132/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.4162 - acc: 0.3903\nEpoch 133/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.3877 - acc: 0.3974\nEpoch 134/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 2.3819 - acc: 0.3966\nEpoch 135/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 2.3547 - acc: 0.4064\nEpoch 136/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 2.3469 - acc: 0.3992\nEpoch 137/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.3291 - acc: 0.4107\nEpoch 138/300\n11368/11368 [==============================] - 18s 2ms/step - loss: 2.3103 - acc: 0.4192\nEpoch 139/300\n11368/11368 [==============================] - 28s 2ms/step - loss: 2.3153 - acc: 0.4086\nEpoch 140/300\n11368/11368 [==============================] - 26s 2ms/step - loss: 2.2902 - acc: 0.4207\nEpoch 141/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.2683 - acc: 0.4208\nEpoch 142/300\n11368/11368 [==============================] - 16s 1ms/step - loss: 2.2673 - acc: 0.4251\nEpoch 143/300\n11368/11368 [==============================] - 17s 1ms/step - loss: 2.2603 - acc: 0.4229\nEpoch 144/300\n11368/11368 [==============================] - 17s 1ms/step - loss: 2.2370 - acc: 0.4302\nEpoch 145/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.2141 - acc: 0.4353\nEpoch 146/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.2047 - acc: 0.4348\nEpoch 147/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.2021 - acc: 0.4335\nEpoch 148/300\n11368/11368 [==============================] - 18s 2ms/step - loss: 2.1856 - acc: 0.4352\nEpoch 149/300\n11368/11368 [==============================] - 17s 1ms/step - loss: 2.1559 - acc: 0.4494\nEpoch 150/300\n11368/11368 [==============================] - 16s 1ms/step - loss: 2.1451 - acc: 0.4533\nEpoch 151/300\n11368/11368 [==============================] - 16s 1ms/step - loss: 2.1348 - acc: 0.4539\nEpoch 152/300\n11368/11368 [==============================] - 16s 1ms/step - loss: 2.1153 - acc: 0.4578\nEpoch 153/300\n11368/11368 [==============================] - 17s 2ms/step - loss: 2.1079 - acc: 0.4564\nEpoch 154/300\n11368/11368 [==============================] - 17s 2ms/step - loss: 2.0943 - acc: 0.4609\nEpoch 155/300\n11368/11368 [==============================] - 17s 2ms/step - loss: 2.0883 - acc: 0.4631\nEpoch 156/300\n11368/11368 [==============================] - 18s 2ms/step - loss: 2.0717 - acc: 0.4654\nEpoch 157/300\n11368/11368 [==============================] - 17s 2ms/step - loss: 2.0601 - acc: 0.4687\nEpoch 158/300\n11368/11368 [==============================] - 17s 1ms/step - loss: 2.0450 - acc: 0.4753\nEpoch 159/300\n11368/11368 [==============================] - 16s 1ms/step - loss: 2.0311 - acc: 0.4769\nEpoch 160/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 2.0277 - acc: 0.4762\nEpoch 161/300\n11368/11368 [==============================] - 18s 2ms/step - loss: 2.0194 - acc: 0.4779\nEpoch 162/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 2.0138 - acc: 0.4712\nEpoch 163/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.9901 - acc: 0.4862\nEpoch 164/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.9795 - acc: 0.4887\nEpoch 165/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 1.9689 - acc: 0.4904\nEpoch 166/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.9482 - acc: 0.4960\nEpoch 167/300\n11368/11368 [==============================] - 19s 2ms/step - loss: 1.9453 - acc: 0.4935\nEpoch 168/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.9308 - acc: 0.4989\nEpoch 169/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.9163 - acc: 0.5004\nEpoch 170/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.9060 - acc: 0.5059\nEpoch 171/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8911 - acc: 0.5060\nEpoch 172/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8828 - acc: 0.5121\nEpoch 173/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8720 - acc: 0.5101\nEpoch 174/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8722 - acc: 0.5135\nEpoch 175/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8549 - acc: 0.5182\nEpoch 176/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8516 - acc: 0.5214\nEpoch 177/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8185 - acc: 0.5253\nEpoch 178/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8136 - acc: 0.5293\nEpoch 179/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8236 - acc: 0.5206\nEpoch 180/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.8208 - acc: 0.5212\nEpoch 181/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7938 - acc: 0.5283\nEpoch 182/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.7772 - acc: 0.5369\nEpoch 183/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7662 - acc: 0.5391\nEpoch 184/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7472 - acc: 0.5450\nEpoch 185/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7394 - acc: 0.5453\nEpoch 186/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7324 - acc: 0.5445\nEpoch 187/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.7324 - acc: 0.5455\nEpoch 188/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7198 - acc: 0.5492\nEpoch 189/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.7110 - acc: 0.5479\nEpoch 190/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.7023 - acc: 0.5527\nEpoch 191/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.6939 - acc: 0.5562\nEpoch 192/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.6732 - acc: 0.5614\nEpoch 193/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.6592 - acc: 0.5669\nEpoch 194/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.6554 - acc: 0.5648\nEpoch 195/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.6960 - acc: 0.5495\nEpoch 196/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.6646 - acc: 0.5563\nEpoch 197/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.6371 - acc: 0.5685\nEpoch 198/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.6125 - acc: 0.5744\nEpoch 199/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.6029 - acc: 0.5797\nEpoch 200/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5937 - acc: 0.5800\nEpoch 201/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5980 - acc: 0.5771\nEpoch 202/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5795 - acc: 0.5830\nEpoch 203/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5719 - acc: 0.5874\nEpoch 204/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5531 - acc: 0.5897\nEpoch 205/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5480 - acc: 0.5954\nEpoch 206/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5438 - acc: 0.5912\nEpoch 207/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5484 - acc: 0.5909\nEpoch 208/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5295 - acc: 0.5956\nEpoch 209/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5142 - acc: 0.6001\nEpoch 210/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4905 - acc: 0.6068\nEpoch 211/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.5008 - acc: 0.6037\nEpoch 212/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4965 - acc: 0.6020\nEpoch 213/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4888 - acc: 0.6075\nEpoch 214/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4781 - acc: 0.6086\nEpoch 215/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4554 - acc: 0.6137\nEpoch 216/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.4417 - acc: 0.6193\nEpoch 217/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.4482 - acc: 0.6158\nEpoch 218/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4439 - acc: 0.6199\nEpoch 219/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4237 - acc: 0.6207\nEpoch 220/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4156 - acc: 0.6215\nEpoch 221/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4145 - acc: 0.6285\nEpoch 222/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.4086 - acc: 0.6296\nEpoch 223/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3904 - acc: 0.6324\nEpoch 224/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.3774 - acc: 0.6335\nEpoch 225/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3709 - acc: 0.6394\nEpoch 226/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3638 - acc: 0.6362\nEpoch 227/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3569 - acc: 0.6432\nEpoch 228/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3497 - acc: 0.6391\nEpoch 229/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3317 - acc: 0.6483\nEpoch 230/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.3331 - acc: 0.6500\nEpoch 231/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.3286 - acc: 0.6513\nEpoch 232/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.3154 - acc: 0.6510\nEpoch 233/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.3001 - acc: 0.6548\nEpoch 234/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.2998 - acc: 0.6567\nEpoch 235/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.2914 - acc: 0.6570\nEpoch 236/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.2748 - acc: 0.6603\nEpoch 237/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.2635 - acc: 0.6641\nEpoch 238/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.2729 - acc: 0.6617\nEpoch 239/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.2598 - acc: 0.6643\nEpoch 240/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.2520 - acc: 0.6698\nEpoch 241/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.2478 - acc: 0.6640\nEpoch 242/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.2303 - acc: 0.6784\nEpoch 243/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.2179 - acc: 0.6768\nEpoch 244/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.2131 - acc: 0.6746\nEpoch 245/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.2038 - acc: 0.6812\nEpoch 246/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.1825 - acc: 0.6880\nEpoch 247/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.1777 - acc: 0.6881\nEpoch 248/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.1730 - acc: 0.6920\nEpoch 249/300\n11368/11368 [==============================] - 22s 2ms/step - loss: 1.1616 - acc: 0.6905\nEpoch 250/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.1465 - acc: 0.6961\nEpoch 251/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.1519 - acc: 0.6896\nEpoch 252/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.1356 - acc: 0.6985\nEpoch 253/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.1196 - acc: 0.7072\nEpoch 254/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 1.1291 - acc: 0.6966\nEpoch 255/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.1090 - acc: 0.7050\nEpoch 256/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0982 - acc: 0.7120\nEpoch 257/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.1027 - acc: 0.7089\nEpoch 258/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0869 - acc: 0.7120\nEpoch 259/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0692 - acc: 0.7159\nEpoch 260/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0766 - acc: 0.7159\nEpoch 261/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0637 - acc: 0.7199\nEpoch 262/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0389 - acc: 0.7270\nEpoch 263/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0399 - acc: 0.7281\nEpoch 264/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0413 - acc: 0.7245\nEpoch 265/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0178 - acc: 0.7328\nEpoch 266/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0231 - acc: 0.7247\nEpoch 267/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0106 - acc: 0.7346\nEpoch 268/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9958 - acc: 0.7367\nEpoch 269/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 1.0025 - acc: 0.7371\nEpoch 270/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9810 - acc: 0.7394\nEpoch 271/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9691 - acc: 0.7435\nEpoch 272/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9737 - acc: 0.7440\nEpoch 273/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9839 - acc: 0.7370\nEpoch 274/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9619 - acc: 0.7484\nEpoch 275/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9605 - acc: 0.7423\nEpoch 276/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9448 - acc: 0.7486\nEpoch 277/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9210 - acc: 0.7552\nEpoch 278/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9059 - acc: 0.7595\nEpoch 279/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8967 - acc: 0.7661\nEpoch 280/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8922 - acc: 0.7661\nEpoch 281/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8839 - acc: 0.7690\nEpoch 282/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8985 - acc: 0.7634\nEpoch 283/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.9160 - acc: 0.7549\nEpoch 284/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8919 - acc: 0.7618\nEpoch 285/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8673 - acc: 0.7709\nEpoch 286/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8545 - acc: 0.7756\nEpoch 287/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.8312 - acc: 0.7807\nEpoch 288/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.8140 - acc: 0.7911\nEpoch 289/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.8034 - acc: 0.7953\nEpoch 290/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.8075 - acc: 0.7925\nEpoch 291/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.8027 - acc: 0.7908\nEpoch 292/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8034 - acc: 0.7923\nEpoch 293/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.8092 - acc: 0.7903\nEpoch 294/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.7989 - acc: 0.7931\nEpoch 295/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.7962 - acc: 0.7920\nEpoch 296/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.7839 - acc: 0.7955\nEpoch 297/300\n11368/11368 [==============================] - 20s 2ms/step - loss: 0.7879 - acc: 0.7933\nEpoch 298/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.7845 - acc: 0.7942\nEpoch 299/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.7652 - acc: 0.8024\nEpoch 300/300\n11368/11368 [==============================] - 21s 2ms/step - loss: 0.7379 - acc: 0.8105\n"
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x21908be2908>"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=300,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(model,tokenizer,seq_len,seed_text,num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text],maxlen=seq_len,truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'head i have in the scales of the window and his dark bag into bed but was getting late and what directly thought i heard in me with the floor in one corner it was it may be nothing but this was exactly that ere double duty with the best'"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_text = 'This is a story of a journey'\n",
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "\"that his very large oilpainting so thoroughly besmoked and frozen entry and at last in the unequal crosslights in him they raised a cry of bulkington bulkington of n't make and comfortable would make and when right length my head which you entered a strong say over in the holy\""
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random_pick = random.randint(0,len(text_sequences))\n",
    "random_seed_text = text_sequences[random_pick]\n",
    "seed_text = ' '.join(random_seed_text)\n",
    "\n",
    "generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}